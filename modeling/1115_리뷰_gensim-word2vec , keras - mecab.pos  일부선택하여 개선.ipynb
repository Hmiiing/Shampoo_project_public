{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 추출 - 예제 코드입니다. \n",
    "\n",
    "- 아래 데이터셋은 0이 19000개 1이 500개정도 이루어진 불균형한 레이블 구성의 데이터셋입니다. 어디까지나 테스트용 리뷰셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel('샴푸샴푸세정력.xlsx')\n",
    "data2 = data1.loc[:,['review','pos']]\n",
    "data2.dropna(subset=['review'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn - data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13917, 1), (5965, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = data2['pos']\n",
    "feature_df = data2.drop(['pos'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리뷰데이터 -> 리스트 /// because of vector화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "for review in X_train['review']:\n",
    "    X_train_list.append(review)\n",
    "    \n",
    "X_test_list = []\n",
    "for review in X_test['review']:\n",
    "    X_test_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'넘좋아요\\n저렴하게구입했네요'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec - gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = open('mycorpus_example.txt', mode='r', encoding='utf-8')\n",
    "reviews = r.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://openuiz.blogspot.com/2016/07/mecab-ko-dic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab  \n",
    "mecab=Mecab()\n",
    "#fread = open('mycorpus_example.txt', mode='r', encoding='utf-8')\n",
    "\n",
    "n=0\n",
    "result = []\n",
    "\n",
    "for review in reviews:\n",
    "    tokenlist = mecab.pos(review)\n",
    "    temp=[]\n",
    "    for word in tokenlist:\n",
    "        # NNP: 고유명사, NNG:일반명사, MAG:일반 부사, VA:형용사,EC:연결어미, NR:수사\n",
    "        if word[1] in [\"MAG\",\"NNG\",\"VA+EC\",\"VA\",\"NNG\",\"NR\"]: \n",
    "            temp.append((word[0])) # 해당 단어를 저장함\n",
    "\n",
    "    if temp: # 만약 이번에 읽은 데이터에 명사가 존재할 경우에만\n",
    "        result.append(temp) # 결과에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23056,\n",
       " [['기존', '상품', '대비', '가격', '부담', '할인', '높', '만족', '사용', '정가', '구매', '아쉬워'],\n",
       "  ['뽀득뽀득', '성분', '좋', '샴푸', '거품', '너무', '안', '세정력', '뽀득뽀득', '좋', '좋'],\n",
       "  ['매번', '거품', '풍', '넝', '맘']])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result) ,result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49395 unique tokens.\n",
      "[4346, 210, 265, 4, 114, 28, 3, 1036, 311, 954, 1214, 1096, 2713, 13, 8707, 126, 1]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 가장 빈도가 높은 10,000개의 단어만 선택하도록 Tokenizer 객체를 만듭니다.\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "# 단어 인덱스를 구축합니다.\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "# 문자열을 정수 인덱스의 리스트로 변환합니다.\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "# 직접 원-핫 이진 벡터 표현을 얻을 수 있습니다.\n",
    "# 원-핫 인코딩 외에 다른 벡터화 방법들도 제공합니다!\n",
    "one_hot_results = tokenizer.texts_to_matrix(reviews, mode='count')\n",
    "\n",
    "# 계산된 단어 인덱스를 구합니다.\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(result, size=500, window=3, min_count=10,workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('달라요', 0.9976422786712646), ('전혀', 0.9972014427185059), ('탓', 0.9971702098846436), ('약하', 0.9967210292816162), ('마사지', 0.9963985681533813), ('세정', 0.9958658218383789), ('축', 0.9957591891288757), ('가볍', 0.9956486225128174), ('확', 0.9953948855400085), ('덜하', 0.9951424598693848)]\n"
     ]
    }
   ],
   "source": [
    "# most_similar 함수는 코사인 유사도를 구해줍니다. \n",
    "model_result1=model.wv.most_similar(\"뽀득\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('일단', 0.930509090423584), ('너무', 0.9155619144439697), ('향', 0.9055719375610352), ('요향', 0.9022431373596191), ('참', 0.8925585746765137), ('향기', 0.8913396596908569), ('향도', 0.8912991881370544), ('맘', 0.8906192183494568), ('성분', 0.8752028346061707), ('우선', 0.873571515083313)]\n"
     ]
    }
   ],
   "source": [
    "model_result1=model.wv.most_similar(\"좋\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 모델 저장 및 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('test_01') # 모델 저장\n",
    "loaded_model=KeyedVectors.load_word2vec_format(\"test_01\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = loaded_model.most_similar(\"좋\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. keras - 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('세일', 'NNG'), ('해서', 'XSV+EC'), ('구매', 'NNG'), ('했어요', 'XSV+EP+EC')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab.pos(\"세일해서 구매했어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['최고/NNG',\n",
      "  '바티스트/NNG',\n",
      "  '성비/NNG',\n",
      "  '좋/VA',\n",
      "  '좋/VA',\n",
      "  '드라이/NNG',\n",
      "  '샴푸/NNG',\n",
      "  '없/VA',\n",
      "  '같/VA',\n",
      "  '구매/NNG',\n",
      "  '기억/NNG',\n",
      "  '안/MAG',\n",
      "  '머리/NNG',\n",
      "  '길/VA',\n",
      "  '매일/MAG',\n",
      "  '감기/NNG',\n",
      "  '늘/MAG',\n",
      "  '유용/NNG',\n",
      "  '사용/NNG'],\n",
      " 0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def tokenize(doc):\n",
    "    # NNP: 고유명사, NNG:일반명사, MAG:일반 부사, VA:형용사,EC:연결어미, NR:수사\n",
    "    tokenlist = mecab.pos(doc)\n",
    "    return ['/'.join(word) for word in tokenlist if word[1] in [\"MAG\",\"NNG\",\"VA+EC\",\"VA\",\"NNG\",\"NR\",\"VV\"]]\n",
    "            #[\"MAG\",\"NNG\",\"VA+EC\",\"VA\",\"NNG\",\"NR\"]]\n",
    "\n",
    "if os.path.isfile('train_docs5.json'):\n",
    "    with open('train_docs5.json') as f:\n",
    "        train_docs = json.load(f)\n",
    "    with open('test_docs5.json') as f:\n",
    "        test_docs = json.load(f)\n",
    "else:\n",
    "    train_docs = [(tokenize(row),label ) for row,label in zip(X_train['review'],y_train)]\n",
    "    test_docs = [(tokenize(row),label ) for row,label in zip(X_test['review'],y_test)]\n",
    "    # JSON 파일로 저장\n",
    "    with open('train_docs5.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(str(train_docs), make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "    with open('test_docs5.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "        json.dump(str(test_docs), make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "\n",
    "# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134953\n",
      "<Text: 세정력>\n",
      "134953\n",
      "4670\n",
      "[('좋/VA', 12868),\n",
      " ('샴푸/NNG', 4438),\n",
      " ('구매/NNG', 3675),\n",
      " ('사용/NNG', 3603),\n",
      " ('잘/MAG', 3256),\n",
      " ('향/NNG', 3126),\n",
      " ('같/VA', 3108),\n",
      " ('두피/NNG', 2534),\n",
      " ('제품/NNG', 2362),\n",
      " ('머리/NNG', 2264)]\n"
     ]
    }
   ],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(len(tokens))\n",
    "\n",
    "import nltk\n",
    "text = nltk.Text(tokens,name='세정력')\n",
    "print(text)\n",
    "\n",
    "# 전체 토큰의 개수\n",
    "print(len(text.tokens))\n",
    "\n",
    "# 중복을 제외한 토큰의 개수\n",
    "print(len(set(text.tokens)))            \n",
    "\n",
    "# 출현 빈도가 높은 상위 토큰 10개\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = [f[0] for f in text.vocab().most_common(10000)]\n",
    "\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(d) for d, _ in train_docs]\n",
    "test_x = [term_frequency(d) for d, _ in test_docs]\n",
    "train_y = [c for _, c in train_docs]\n",
    "test_y = [c for _, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.asarray(train_x).astype('float32')\n",
    "x_test = np.asarray(test_x).astype('float32')\n",
    "\n",
    "y_train = np.asarray(train_y).astype('float32')\n",
    "y_test = np.asarray(test_y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:2000]\n",
    "partial_x_train = x_train[2000:]\n",
    "y_val = y_train[:2000]\n",
    "partial_y_train = y_train[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13917 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "13917/13917 [==============================] - 1s 84us/sample - loss: 0.2909 - binary_accuracy: 0.9843 - val_loss: 0.1691 - val_binary_accuracy: 0.9820\n",
      "Epoch 2/10\n",
      "13917/13917 [==============================] - 1s 85us/sample - loss: 0.1272 - binary_accuracy: 0.9846 - val_loss: 0.1004 - val_binary_accuracy: 0.9820\n",
      "Epoch 3/10\n",
      "13917/13917 [==============================] - 1s 80us/sample - loss: 0.0712 - binary_accuracy: 0.9846 - val_loss: 0.0473 - val_binary_accuracy: 0.9820\n",
      "Epoch 4/10\n",
      "13917/13917 [==============================] - 1s 66us/sample - loss: 0.0378 - binary_accuracy: 0.9866 - val_loss: 0.0246 - val_binary_accuracy: 0.9915\n",
      "Epoch 5/10\n",
      "13917/13917 [==============================] - 1s 63us/sample - loss: 0.0231 - binary_accuracy: 0.9925 - val_loss: 0.0162 - val_binary_accuracy: 0.9965\n",
      "Epoch 6/10\n",
      "13917/13917 [==============================] - 1s 70us/sample - loss: 0.0154 - binary_accuracy: 0.9955 - val_loss: 0.0108 - val_binary_accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "13917/13917 [==============================] - 1s 68us/sample - loss: 0.0111 - binary_accuracy: 0.9963 - val_loss: 0.0074 - val_binary_accuracy: 0.9980\n",
      "Epoch 8/10\n",
      "13917/13917 [==============================] - 1s 69us/sample - loss: 0.0082 - binary_accuracy: 0.9971 - val_loss: 0.0053 - val_binary_accuracy: 0.9985\n",
      "Epoch 9/10\n",
      "13917/13917 [==============================] - 1s 68us/sample - loss: 0.0063 - binary_accuracy: 0.9980 - val_loss: 0.0042 - val_binary_accuracy: 0.9990\n",
      "Epoch 10/10\n",
      "13917/13917 [==============================] - 1s 67us/sample - loss: 0.0046 - binary_accuracy: 0.9989 - val_loss: 0.0036 - val_binary_accuracy: 0.9990\n",
      "5965/5965 [==============================] - 1s 85us/sample - loss: 0.0791 - binary_accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(4670,))) # ERROR 발생시 - inputshape 맞춰주라는 숫자로 바꿔주세요 \n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss=losses.binary_crossentropy,\n",
    "             metrics=[metrics.binary_accuracy])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=512,validation_data=(x_val,y_val))\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07912666127738205, 0.98826486]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos_neg(review):\n",
    "    token = tokenize(review)\n",
    "    tf = term_frequency(token)\n",
    "    data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)\n",
    "    score = float(model.predict(data))\n",
    "    if(score > 0.5):\n",
    "        print(\"[{}]는 {:.2f}% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\\n\".format(review, score * 100))\n",
    "    else:\n",
    "        print(\"[{}]는 {:.2f}% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\\n\".format(review, (1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[뽀득하고 세정이 잘 되는거 같아요.]는 53.76% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_pos_neg(\"뽀득하고 세정이 잘 되는거 같아요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[세정력 좋아요 향은 별로네요 두피가 시원해요]는 54.78% 확률로 긍정 리뷰이지 않을까 추측해봅니다.^^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_pos_neg(\"세정력 좋아요 향은 별로네요 두피가 시원해요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[향이 좋습니다 세정력은 별로에요 이제 안써요]는 99.73% 확률로 부정 리뷰이지 않을까 추측해봅니다.^^;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_pos_neg(\"향이 좋습니다 세정력은 별로에요 이제 안써요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Traning loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
